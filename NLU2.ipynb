{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (second assignment NLU)\n",
    "\n",
    "* Student name: Gaia Trebucchi\n",
    "* Student number: 224464\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load `en_core_web_sm` with `spacy.load`. This will return a `Language` object stored as `nlp_spacy` containing all components and data needed to process text. We also import `pandas` to create tables and `classification_report` from `sklearn.metrics` to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "nlp_spacy=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the module `conll.py` provided at the following link https://github.com/esrel/NLU.Lab.2021/blob/master/src/conll.py is inserted, in order to avoid the necessity of importing it. From this module the function `read_corpus_conll` will be used as starting point to load the dataset and the function `evaluate` will be used for the first and the third request in order to report the precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\"\"\"\n",
    "Modified version of https://pypi.org/project/conlleval/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def stats():\n",
    "    return {'cor': 0, 'hyp': 0, 'ref': 0}\n",
    "\n",
    "\n",
    "def evaluate(ref, hyp, otag='O'):\n",
    "    # evaluation for NLTK\n",
    "    aligned = align_hyp(ref, hyp)\n",
    "    return conlleval(aligned, otag=otag)\n",
    "\n",
    "\n",
    "def align_hyp(ref, hyp):\n",
    "    # align references and hypotheses for evaluation\n",
    "    # add last element of token tuple in hyp to ref\n",
    "    if len(ref) != len(hyp):\n",
    "        raise ValueError(\"Size Mismatch: ref: {} & hyp: {}\".format(len(ref), len(hyp)))\n",
    "\n",
    "    out = []\n",
    "    for i in range(len(ref)):\n",
    "        if len(ref[i]) != len(hyp[i]):\n",
    "            raise ValueError(\"Size Mismatch: ref: {} & hyp: {}\".format(len(ref), len(hyp)))\n",
    "        out.append([(*ref[i][j], hyp[i][j][-1]) for j in range(len(ref[i]))])\n",
    "    return out\n",
    "\n",
    "\n",
    "def conlleval(data, otag='O'):\n",
    "    # token, segment & class level counts for TP, TP+FP, TP+FN\n",
    "    tok = stats()\n",
    "    seg = stats()\n",
    "    cls = {}\n",
    "\n",
    "    for sent in data:\n",
    "\n",
    "        prev_ref = otag      # previous reference label\n",
    "        prev_hyp = otag      # previous hypothesis label\n",
    "        prev_ref_iob = None  # previous reference label IOB\n",
    "        prev_hyp_iob = None  # previous hypothesis label IOB\n",
    "\n",
    "        in_correct = False  # currently processed chunks is correct until now\n",
    "\n",
    "        for token in sent:\n",
    "\n",
    "            hyp_iob, hyp = parse_iob(token[-1])\n",
    "            ref_iob, ref = parse_iob(token[-2])\n",
    "\n",
    "            ref_e = is_eoc(ref, ref_iob, prev_ref, prev_ref_iob, otag)\n",
    "            hyp_e = is_eoc(hyp, hyp_iob, prev_hyp, prev_hyp_iob, otag)\n",
    "\n",
    "            ref_b = is_boc(ref, ref_iob, prev_ref, prev_ref_iob, otag)\n",
    "            hyp_b = is_boc(hyp, hyp_iob, prev_hyp, prev_hyp_iob, otag)\n",
    "\n",
    "            if not cls.get(ref) and ref:\n",
    "                cls[ref] = stats()\n",
    "\n",
    "            if not cls.get(hyp) and hyp:\n",
    "                cls[hyp] = stats()\n",
    "\n",
    "            # segment-level counts\n",
    "            if in_correct:\n",
    "                if ref_e and hyp_e and prev_hyp == prev_ref:\n",
    "                    in_correct = False\n",
    "                    seg['cor'] += 1\n",
    "                    cls[prev_ref]['cor'] += 1\n",
    "\n",
    "                elif ref_e != hyp_e or hyp != ref:\n",
    "                    in_correct = False\n",
    "\n",
    "            if ref_b and hyp_b and hyp == ref:\n",
    "                in_correct = True\n",
    "\n",
    "            if ref_b:\n",
    "                seg['ref'] += 1\n",
    "                cls[ref]['ref'] += 1\n",
    "\n",
    "            if hyp_b:\n",
    "                seg['hyp'] += 1\n",
    "                cls[hyp]['hyp'] += 1\n",
    "\n",
    "            # token-level counts\n",
    "            if ref == hyp and ref_iob == hyp_iob:\n",
    "                tok['cor'] += 1\n",
    "\n",
    "            tok['ref'] += 1\n",
    "\n",
    "            prev_ref = ref\n",
    "            prev_hyp = hyp\n",
    "            prev_ref_iob = ref_iob\n",
    "            prev_hyp_iob = hyp_iob\n",
    "\n",
    "        if in_correct:\n",
    "            seg['cor'] += 1\n",
    "            cls[prev_ref]['cor'] += 1\n",
    "\n",
    "    return summarize(seg, cls)\n",
    "\n",
    "\n",
    "def parse_iob(t):\n",
    "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
    "    return m.groups() if m else (t, None)\n",
    "\n",
    "\n",
    "def is_boc(lbl, iob, prev_lbl, prev_iob, otag='O'):\n",
    "    \"\"\"\n",
    "    is beginning of a chunk\n",
    "    supports: IOB, IOBE, BILOU schemes\n",
    "        - {E,L} --> last\n",
    "        - {S,U} --> unit\n",
    "    :param lbl: current label\n",
    "    :param iob: current iob\n",
    "    :param prev_lbl: previous label\n",
    "    :param prev_iob: previous iob\n",
    "    :param otag: out-of-chunk label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    boc = False\n",
    "\n",
    "    boc = True if iob in ['B', 'S', 'U'] else boc\n",
    "    boc = True if iob in ['E', 'L'] and prev_iob in ['E', 'L', 'S', otag] else boc\n",
    "    boc = True if iob == 'I' and prev_iob in ['S', 'L', 'E', otag] else boc\n",
    "\n",
    "    boc = True if lbl != prev_lbl and iob != otag and iob != '.' else boc\n",
    "\n",
    "    # these chunks are assumed to have length 1\n",
    "    boc = True if iob in ['[', ']'] else boc\n",
    "\n",
    "    return boc\n",
    "\n",
    "\n",
    "def is_eoc(lbl, iob, prev_lbl, prev_iob, otag='O'):\n",
    "    \"\"\"\n",
    "    is end of a chunk\n",
    "    supports: IOB, IOBE, BILOU schemes\n",
    "        - {E,L} --> last\n",
    "        - {S,U} --> unit\n",
    "    :param lbl: current label\n",
    "    :param iob: current iob\n",
    "    :param prev_lbl: previous label\n",
    "    :param prev_iob: previous iob\n",
    "    :param otag: out-of-chunk label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    eoc = False\n",
    "\n",
    "    eoc = True if iob in ['E', 'L', 'S', 'U'] else eoc\n",
    "    eoc = True if iob == 'B' and prev_iob in ['B', 'I'] else eoc\n",
    "    eoc = True if iob in ['S', 'U'] and prev_iob in ['B', 'I'] else eoc\n",
    "\n",
    "    eoc = True if iob == otag and prev_iob in ['B', 'I'] else eoc\n",
    "\n",
    "    eoc = True if lbl != prev_lbl and iob != otag and prev_iob != '.' else eoc\n",
    "\n",
    "    # these chunks are assumed to have length 1\n",
    "    eoc = True if iob in ['[', ']'] else eoc\n",
    "\n",
    "    return eoc\n",
    "\n",
    "\n",
    "def score(cor_cnt, hyp_cnt, ref_cnt):\n",
    "    # precision\n",
    "    p = 1 if hyp_cnt == 0 else cor_cnt / hyp_cnt\n",
    "    # recall\n",
    "    r = 0 if ref_cnt == 0 else cor_cnt / ref_cnt\n",
    "    # f-measure (f1)\n",
    "    f = 0 if p+r == 0 else (2*p*r)/(p+r)\n",
    "    return {\"p\": p, \"r\": r, \"f\": f, \"s\": ref_cnt}\n",
    "\n",
    "\n",
    "def summarize(seg, cls):\n",
    "    # class-level\n",
    "    res = {lbl: score(cls[lbl]['cor'], cls[lbl]['hyp'], cls[lbl]['ref']) for lbl in set(cls.keys())}\n",
    "    # micro\n",
    "    res.update({\"total\": score(seg.get('cor', 0), seg.get('hyp', 0), seg.get('ref', 0))})\n",
    "    return res\n",
    "\n",
    "\n",
    "def read_corpus_conll(corpus_file, fs=\"\\t\"):\n",
    "    \"\"\"\n",
    "    read corpus in CoNLL format\n",
    "    :param corpus_file: corpus in conll format\n",
    "    :param fs: field separator\n",
    "    :return: corpus\n",
    "    \"\"\"\n",
    "    featn = None  # number of features for consistency check\n",
    "    sents = []  # list to hold words list sequences\n",
    "    words = []  # list to hold feature tuples\n",
    "\n",
    "    for line in open(corpus_file):\n",
    "        line = line.strip()\n",
    "        if len(line.strip()) > 0:\n",
    "            feats = tuple(line.strip().split(fs))\n",
    "            if not featn:\n",
    "                featn = len(feats)\n",
    "            elif featn != len(feats) and len(feats) != 0:\n",
    "                raise ValueError(\"Unexpected number of columns {} ({})\".format(len(feats), featn))\n",
    "\n",
    "            words.append(feats)\n",
    "        else:\n",
    "            if len(words) > 0:\n",
    "                sents.append(words)\n",
    "                words = []\n",
    "    return sents\n",
    "\n",
    "\n",
    "def get_chunks(corpus_file, fs=\"\\t\", otag=\"O\"):\n",
    "    sents = read_corpus_conll(corpus_file, fs=fs)\n",
    "    return set([parse_iob(token[-1])[1] for sent in sents for token in sent if token[-1] != otag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request 1: \n",
    "#### Evaluate spaCy NER on CoNLL 2003 data (provided)\n",
    "* **1.1:** Report token-level performance (per class and total): accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy)\n",
    "* **1.2:** Report CoNLL chunk-level performance (per class and total): precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's analyze the function `get_sentences`:\n",
    "* **input**: the CoNLL file name from which we want to extract the true named entity labels\n",
    "* **output**: \n",
    "    * a list containing all the sentences reconstructed by the dataset\n",
    "    * a list of tuples, where the first element of the tuple is the text of the token and the second element is the named entity label in the CoNLL format extracted from the dataset file\n",
    "    \n",
    "First, the provided function `read_corpus_conll` from `conll.py` is called to store each sentence in a list of tuples, each containing a single line (corresponding to a single token and the referred labels) of the text.\n",
    "Then, two list are created to store both the string sentences and the true named entity labels extracted from the file. For each line in the output of `read_corpus_conll` the function behaves in that way:\n",
    "* a string is created to store the text of the tokens inside the sentence under observation and a list is created to store the pairs [token text, named entity label]\n",
    "* for each tuple (containing all the information about a single token) a list containing the information is created. If the first element of the list (the element that represents the token text) is different from '-DOCSTART-', a tuple containing the token text and its name entity label is added to `conll_sentence` list and the first element (the token text) is also added to the `sentence` along with a space to recreate the sentence.\n",
    "* If the sentence is not empty the string sentence and the list of conll tuples are respectively added to `sentences` list and `conll` list as new elements. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file):\n",
    "    sents=read_corpus_conll(file)\n",
    "    sentences=[]\n",
    "    conll=[]\n",
    "    for sent in sents:\n",
    "        sentence=\"\"\n",
    "        conll_sentence=[]\n",
    "        for tupl in sent:\n",
    "            list_conll=(tupl[0]).split()\n",
    "            if list_conll[0]!='-DOCSTART-':\n",
    "                conll_sentence.append((list_conll[0],list_conll[-1]))\n",
    "                sentence+=list_conll[0]+\" \"\n",
    "        if len(sentence)!=0:\n",
    "            sentences.append(sentence)\n",
    "            conll.append(conll_sentence)\n",
    "    return sentences, conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The funcion `post_fix_tokenization` is used to deal with the different segmentation done by spacy parser with respect to the dataset segmentation of tokens. It also deals with the compound dependency issue.\n",
    "* **input**: a sentence passed as string and the True/False boolean referred to the compound analysis\n",
    "* **output**: a list of lists matching the input of the successive `spacy_to_conll` function\n",
    "\n",
    "First, the doc object of the processed sentence is created by `nlp_spacy` and the `new_list` output list is created. \n",
    "For each token, the function cycles in that way:\n",
    "* the first step is to merge token text separated by spacy parser but considered together in the CoNLL dataset. To do that the token attribute whitespace is used: if there wasn't a space between two successive tokens, the text of the second token is added to the text of the previous one. In the same cycle, a list consisting of the text of the token, the entity iob label, the named entity label and the token itself is created and added to the `new_list` list of lists. If `compound` is set to False in the input, the `new_list` list of lists is returned by the function.\n",
    "* Otherwise, if `compound` is set to True in the input, the `new_list` list of lists is fed as input to the `get_compound` function explained in detail below (Request 3) and the output of `get_compound(new_list,doc)`, having the same structure of `new_list`, is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_fix_tokenization(sentence,compound=False):\n",
    "    doc=nlp_spacy(sentence)\n",
    "    new_list=[]\n",
    "    for tokene in doc:\n",
    "        index=tokene.i\n",
    "        if index!=0 and doc[index-1].whitespace_=='':\n",
    "            new_list[-1][0]+=tokene.text\n",
    "        else:\n",
    "            new_list.append([tokene.text,tokene.ent_iob_,tokene.ent_type_,tokene])\n",
    "    if compound==False:\n",
    "        return new_list\n",
    "    else:\n",
    "        new_list1=get_compound(new_list,doc)\n",
    "        return new_list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `spacy_to_conll` function is made to re-label the named entity recognized in the CoNLL format starting from the NER labels of spacy. \n",
    "* **input**: a list of lists, where the outer list is referred to a sentence and the inner lists are referred to the tokens of the sentence and contain, in order, the token text, the token iob label of spacy, the token entity label of spacy and the token itself\n",
    "* **output**: a list of tuples where the first element is the token text and the second element is the name entity label of the token obtained by converting the spacy named entity label to the CoNLL nemed entity label that more represents it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_to_conll(list_of_tok):\n",
    "    new_list=[]\n",
    "    for token in list_of_tok:\n",
    "        if token[2]=='ORG':\n",
    "            new_list.append((token[0],token[1]+\"-\"+token[2]))\n",
    "        elif token[2]=='PERSON':\n",
    "            new_list.append((token[0], token[1]+\"-\"+'PER'))\n",
    "        elif token[2]=='GPE' or token[2]=='LOC' or token[2]=='FAC':\n",
    "            new_list.append((token[0], token[1]+\"-\"+'LOC'))\n",
    "        elif token[2]=='NORP' or token[2]=='PRODUCT' or token[2]=='EVENT' or token[2]=='WORK_OF_ART' or token[2]=='LANGUAGE' or token[2]=='TIME':\n",
    "            new_list.append((token[0],token[1]+\"-\"+'MISC'))  \n",
    "        else:\n",
    "            new_list.append((token[0], 'O'))\n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `my_conll` is used to combine `spacy_to_conll` and `post_fix_tokenization`, to set to True/False the compound analysis and finally  obtain the prediction of spacy for all the tokens and all the sentences of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conll(text,compound=False):\n",
    "    my_conll=[]\n",
    "    if compound==False:\n",
    "        for sent in text:\n",
    "            my_conll.append(spacy_to_conll(post_fix_tokenization(sent)))\n",
    "    else:\n",
    "        for sent in text:\n",
    "            my_conll.append(spacy_to_conll(post_fix_tokenization(sent,True)))\n",
    "    return my_conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we extract the prediction for the test set and the train set of CoNLL 2003 dataset. The prediction are stored in a list of lists (output of `my_conll`), where the outer list represents the entire document, and the inner lists represent the sentences according to the division done by `read_corpus_conll`. The elements of the inner lists are tuples whose first element is the token text and the second element is the named entity prediction by spacy NER (then post-processed by the functions described above to match the token segmentation in the dataset and the name entity labels). With `get_sentences` we exctract both the list of sentences to feed `my_conll` function and the true named entity labels (the structure of this output is the same as the output of `my_conll`, a list of lists where the elements of the inner lists are tuples whose first element is the token text and the second element is the true named entity label extract by the ConLL 2003 dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test,conll_test=get_sentences(\"test.txt\")\n",
    "my_conll_test=my_conll(sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train,conll_train=get_sentences(\"train.txt\")\n",
    "my_conll_train=my_conll(sentences_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To report token-level performances (per class and total) the scikit-learn module is used to compute the accuracy of correctly recognizing all the tokens that belong to named entities (i.e. tag-level accuracy).\n",
    "The function `accuracy_token` creates the array of labels to be compared by `classification_report` from sklearn by extracting them from the true labels and the array of the prediction of spacy named entity recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_token(my_conll,true_conll):\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    for i in range(0,len(my_conll)):\n",
    "        for j in range(0,len(my_conll[i])):\n",
    "            y_true.append(true_conll[i][j][1])\n",
    "            y_pred.append(my_conll[i][j][1])\n",
    "    return y_true,y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate here a report on the test set of CoNLL 2003:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.76      0.68      0.72      1668\n",
      "       B-PER       0.65      0.56      0.60       702\n",
      "       I-PER       0.52      0.31      0.38      1661\n",
      "       B-ORG       0.80      0.63      0.70      1617\n",
      "       I-ORG       0.54      0.56      0.55       257\n",
      "       B-LOC       0.26      0.36      0.30       216\n",
      "       I-LOC       0.42      0.51      0.46       835\n",
      "      B-MISC       0.84      0.79      0.81      1156\n",
      "      I-MISC       0.95      0.98      0.96     38323\n",
      "\n",
      "    accuracy                           0.90     46435\n",
      "   macro avg       0.64      0.60      0.61     46435\n",
      "weighted avg       0.90      0.90      0.90     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = [ 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', \n",
    "        'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "y_true,y_pred=accuracy_token(my_conll_test,conll_test)\n",
    "print(classification_report(y_true, y_pred,target_names=classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report of the train set of ConLL 2003:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.79      0.69      0.74      7140\n",
      "       B-PER       0.69      0.57      0.62      3438\n",
      "       I-PER       0.48      0.32      0.38      6321\n",
      "       B-ORG       0.80      0.67      0.73      6600\n",
      "       I-ORG       0.57      0.61      0.59      1157\n",
      "       B-LOC       0.23      0.22      0.23      1155\n",
      "       I-LOC       0.49      0.55      0.52      3704\n",
      "      B-MISC       0.82      0.82      0.82      4528\n",
      "      I-MISC       0.96      0.98      0.97    169578\n",
      "\n",
      "    accuracy                           0.91    203621\n",
      "   macro avg       0.65      0.60      0.62    203621\n",
      "weighted avg       0.91      0.91      0.91    203621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true_train,y_pred_train=accuracy_token(my_conll_train,conll_train)\n",
    "print(classification_report(y_true_train, y_pred_train, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To report CoNLL chunk-level performance (per class and total) the `evaluate` function provided by the `conll.py` module is used to compute the precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total. The references are the true labels of the test set (and successively train set) and the hypothesis are the labels obtain by spacy NER (with all the post-processing explained above) on the test set (and successively train set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.638614</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.591743</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.774193</td>\n",
       "      <td>0.608534</td>\n",
       "      <td>0.681440</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.747832</td>\n",
       "      <td>0.672062</td>\n",
       "      <td>0.707925</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.464105</td>\n",
       "      <td>0.276340</td>\n",
       "      <td>0.346415</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.676060</td>\n",
       "      <td>0.522486</td>\n",
       "      <td>0.589434</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p         r         f     s\n",
       "MISC   0.638614  0.551282  0.591743   702\n",
       "PER    0.774193  0.608534  0.681440  1617\n",
       "LOC    0.747832  0.672062  0.707925  1668\n",
       "ORG    0.464105  0.276340  0.346415  1661\n",
       "total  0.676060  0.522486  0.589434  5648"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs_test=conll_test\n",
    "hyps_test=my_conll_test\n",
    "results_test = evaluate(refs_test, hyps_test)\n",
    "pd_tbl_test = pd.DataFrame().from_dict(results_test, orient='index')\n",
    "pd_tbl_test.round(decimals=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.669024</td>\n",
       "      <td>0.550320</td>\n",
       "      <td>0.603894</td>\n",
       "      <td>3438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.781841</td>\n",
       "      <td>0.651061</td>\n",
       "      <td>0.710483</td>\n",
       "      <td>6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.782330</td>\n",
       "      <td>0.684594</td>\n",
       "      <td>0.730206</td>\n",
       "      <td>7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.428332</td>\n",
       "      <td>0.283183</td>\n",
       "      <td>0.340952</td>\n",
       "      <td>6321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.686203</td>\n",
       "      <td>0.547555</td>\n",
       "      <td>0.609089</td>\n",
       "      <td>23499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p         r         f      s\n",
       "MISC   0.669024  0.550320  0.603894   3438\n",
       "PER    0.781841  0.651061  0.710483   6600\n",
       "LOC    0.782330  0.684594  0.730206   7140\n",
       "ORG    0.428332  0.283183  0.340952   6321\n",
       "total  0.686203  0.547555  0.609089  23499"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs_train=conll_train\n",
    "hyps_train=my_conll_train\n",
    "results_train = evaluate(refs_train, hyps_train)\n",
    "pd_tbl_train = pd.DataFrame().from_dict(results_train, orient='index')\n",
    "pd_tbl_train.round(decimals=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request 2:\n",
    "#### Grouping of Entities\n",
    "Write a function to group recognized named entities using noun_chunks method of spaCy. Analyze the groups in terms of most frequent combinations (i.e. NER types that go together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1:** **Grouping entities**\n",
    "* **input**: a list of sentences passed as string\n",
    "* **output**: a list of lists, where the inner lists have as elements the label of the entities that belongs to the same noun chunk.\n",
    "\n",
    "First, the function creates a dictionary to store the chunks of the text sentences and initializes a list (`chunk_done`) to store the entity chunks already added to the final output and a list (`group_ent`) to store the future output. \n",
    "Then the function cycles across all the sentences belonging to the text.\n",
    "The first step is to obtain a processed `Doc` of the sentence by using `nlp_spacy`. Then there are two different cycles:\n",
    "\n",
    "* The first cycle acts on the doc chunks found by the attribute `doc.noun_chunks` in that way: A list is created and for all the tokens belonging to the chunk under examination there is a check on the entity label. If the token doesn't belong to an entity or if the iob label is different from \"B\" (in other words, if the entity was already detected by the previous token belonging to it) the function skip to the next token, otherwise the entity type encountered is added to the list referred to that noun chunk. After the inspection of all the tokens of the noun chunks, each of the token is added to the `chunks` dictionary as key, and the relative value is a list containing as first element the list of the entity belonging to the token chunk and as second element the chunk containing the key token. \n",
    "* The second cycle acts on the doc entities found by the attribute `doc.ents` in that way: if the first element of the entity (the token with \"B\" iob label) is one of the keys of the dictionary and the chunk it belongs to hasn't been added yet to the list of the grouped entity (this control is made by searching for that chunk in the `chunk_done` list) the list of all the entity labels of its noun chunk (the first element of the token value in the dictionary) is added to the `group_ent` list and the relative chunk is added to the `chunk_done` list in order not to be repeated when finding the other tokens belonging to that chunk. Otherwise, if the first element of the entity doesn't appear in the dictionary keys, its entity type is added to the `group_ent` list as a single element.\n",
    "\n",
    "At the end, the list with the grouped entities is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_entities(text):\n",
    "    chunks=dict()\n",
    "    chunk_done=[]\n",
    "    group_ent=[]\n",
    "    for sentence in text:\n",
    "        doc=nlp_spacy(sentence)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            l=[]\n",
    "            for c in chunk:\n",
    "                if c.ent_type_!=\"\" and c.ent_iob_=='B':\n",
    "                    l.append(c.ent_type_)\n",
    "            for ch in chunk:\n",
    "                chunks[ch]=[l, chunk]\n",
    "        for ent in doc.ents:\n",
    "            if ent[0] in chunks.keys() and chunks[ent[0]][1] not in chunk_done:\n",
    "                group_ent.append(chunks[ent[0]][0])\n",
    "                chunk_done.append(chunks[ent[0]][1])\n",
    "            elif ent[0] not in chunks.keys():\n",
    "                group_ent.append([ent[0].ent_type_])\n",
    "    return group_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1: Frequency analysis**\n",
    "\n",
    "The `nbest` function will return the n more frequent grouped entities in decremental order.\n",
    "The `frequency_NER` function works in that way:\n",
    "* **input**: the list of grouped entities (output of the previous  `grouping_entities` function) and an integer n (setted equal to 100) to choose the number of elements to consider\n",
    "* **output**: the n more frequent grouped entities stored in a dictionary where the keys are the entity labels that belong to the same group and the value for each key is the number of time that the combination of that entities is found.\n",
    "For each group of entities the string representing all the entities is created. If that group hasn't been found yet the string is added to the dictionary as key and the value is setted to one. Otherwise, if the group was already in the dictionary keys, the relative value is incremented of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbest(d,n):\n",
    "    return dict(sorted(d.items(),key=lambda item: item[1],reverse=True)[:n])\n",
    "\n",
    "def frequency_NER(grouped_entity, n=100):\n",
    "    freq=dict()\n",
    "    for gr in grouped_entity:\n",
    "        string=gr[0]\n",
    "        for j in range(1,len(gr)):\n",
    "            string+=\", \"+str(gr[j])\n",
    "        if string not in freq.keys():\n",
    "                freq[string]=1\n",
    "        else:\n",
    "            freq[string]+=1\n",
    "    return(nbest(freq,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of `grouping_entities` and `frequency_NER` with a simple sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ORG', 'PERSON'], ['DATE'], ['GPE'], ['GPE']]\n",
      "{'GPE': 2, 'ORG, PERSON': 1, 'DATE': 1}\n"
     ]
    }
   ],
   "source": [
    "print(grouping_entities([\"Apple's Steve Jobs died in 2011 in Palo Alto, California.\"]))\n",
    "print(frequency_NER(grouping_entities([\"Apple's Steve Jobs died in 2011 in Palo Alto, California\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of `frequency_NER` with the test set of CoNLL 2003 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CARDINAL': 1622, 'GPE': 1253, 'PERSON': 1069, 'DATE': 996, 'ORG': 865, 'NORP': 293, 'MONEY': 147, 'ORDINAL': 111, 'TIME': 83, 'PERCENT': 80, 'EVENT': 58, 'LOC': 54, 'CARDINAL, PERSON': 52, 'QUANTITY': 51, 'NORP, PERSON': 43, 'GPE, PERSON': 34, 'GPE, GPE': 26, 'FAC': 22, 'PRODUCT': 22, 'ORG, PERSON': 21, 'CARDINAL, ORG': 20, 'GPE, ORG': 15, 'CARDINAL, NORP': 15, 'CARDINAL, GPE': 13, 'LAW': 11, 'WORK_OF_ART': 9, 'ORG, ORG': 9, 'GPE, PRODUCT': 9, 'DATE, EVENT': 8, 'DATE, ORG': 8, 'PERSON, PERSON': 8, 'NORP, ORG': 8, 'ORG, DATE': 7, 'DATE, TIME': 7, 'LANGUAGE': 6, 'GPE, DATE': 5, 'CARDINAL, CARDINAL': 5, 'NORP, ORDINAL': 5, 'ORG, GPE': 5, 'DATE, NORP': 5, 'GPE, ORDINAL': 4, 'ORDINAL, PERSON': 4, 'GPE, CARDINAL': 4, 'ORG, NORP': 4, 'PERSON, GPE': 4, 'CARDINAL, DATE': 3, 'PERSON, ORG': 3, 'ORG, CARDINAL': 3, 'CARDINAL, PERSON, CARDINAL': 3, 'NORP, NORP': 3, 'PERSON, PERSON, PERSON': 2, 'CARDINAL, ORDINAL': 2, 'CARDINAL, CARDINAL, PERSON': 2, 'ORG, ORDINAL': 2, 'LANGUAGE, ORDINAL': 2, 'GPE, DATE, ORG': 2, 'ORDINAL, EVENT': 2, 'GPE, LOC': 2, 'CARDINAL, CARDINAL, ORG': 2, 'QUANTITY, QUANTITY': 2, 'GPE, NORP': 2, 'DATE, CARDINAL': 2, 'DATE, NORP, PERSON': 2, 'EVENT, CARDINAL': 2, 'GPE, FAC': 2, 'PERSON, CARDINAL': 2, 'ORDINAL, NORP': 1, 'GPE, PERSON, CARDINAL': 1, 'PERSON, WORK_OF_ART': 1, 'ORDINAL, DATE': 1, 'ORG, GPE, ORDINAL': 1, 'ORG, QUANTITY': 1, 'CARDINAL, GPE, GPE': 1, 'PERCENT, CARDINAL': 1, 'PERCENT, PERSON': 1, 'NORP, DATE': 1, 'PERSON, NORP': 1, 'LOC, DATE': 1, 'DATE, FAC': 1, 'CARDINAL, CARDINAL, NORP': 1, 'NORP, PERSON, DATE': 1, 'LOC, ORDINAL': 1, 'PRODUCT, GPE': 1, 'MONEY, ORG': 1, 'NORP, LOC': 1, 'ORDINAL, GPE': 1, 'MONEY, DATE': 1, 'CARDINAL, PERCENT': 1, 'DATE, WORK_OF_ART': 1, 'MONEY, MONEY': 1, 'MONEY, CARDINAL, CARDINAL, ORG': 1, 'CARDINAL, GPE, TIME': 1, 'FAC, GPE': 1, 'ORG, WORK_OF_ART': 1, 'GPE, ORDINAL, PERSON': 1, 'CARDINAL, LOC': 1, 'PERSON, MONEY': 1, 'MONEY, PRODUCT': 1, 'DATE, PERSON': 1, 'PERSON, GPE, CARDINAL': 1}\n"
     ]
    }
   ],
   "source": [
    "group=grouping_entities(sentences_test)\n",
    "print(frequency_NER(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request 3:\n",
    "One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation.\n",
    "\n",
    "Let's start explaining the `get_compound` function. The goal of this function is to extend the named entity span to the token that have not an entity label but result in compound dependency with a token that is inside an entity.\n",
    "* **input**: a list of lists where the outer list represents a sentence and the inner lists are referred to a token and contain the text of the token, the iob label, the named entity label and the token itself. The second input is the doc object obtained by parsing with `nlp_spacy` the sentence from which the tokens are extracted.  \n",
    "* **output**: the structure of the output is the same as the input, but the iob labels and the named entity labels of the tokens are changed (if necessary) in order to consider the compound dependencies.\n",
    "\n",
    "First, a dictionary is created to store all the tokens that will need a iob label or entity label relabelling. \n",
    "Then, the function cycles across the list of lists of the input in that way:\n",
    "* A check is done on the entity label of the token: we consider only the tokens that have an entity label (if a token is involved in a compound dependency relation with its head but it hasn't an entity label, it will be consider when the cycle will take in observation its head and the `get_compound_path` will be called over it). In addition, another check is done considering two distinct cases: \n",
    "   * if the head of the token appears before the token in the sentence, the token is accepted only if it is not yet been added to the compounds dictionary (if the token is already in the keys of the dictionary, in fact, its relabelling has already be done when the function considered the head of the token, located in a earlier position). If the head of the token appears before in the sentence but the token does not belong yet to the compound dictionary, it means that the token was not reached by the `get_compound_path` function applied to its head, so the token is accepted to get also its compound path. \n",
    "   * if the head of the token appears after the token in the sentence, the token is accepted only if its entity label is different from the entity label of the head (in the case the entity label is equal, the token will appear in the compound path of its head that will be successively considered by `get_compound_path` applied on its head, if instead the entity label is different it will not appear in the compound path of its head, so it is accepted now to explore its compound path). \n",
    "\n",
    "* When a token is taken into consideration after the `if` condition, the `compound_path` list is initialized with the index of the token inside the doc. The `get_compound_path` adds to the path each token index reached by the compound dependency as explained below. Then, the list of the path is sorted in ascendent order of token indexes, to contain the information about the position of the tokens inside the sentence (this information will be useful for the re-labelling of the iob labels). Then, the relabelling is done in that way: if the token corresponding to the first index of `compound_path1` has in the sentence a predecessor that has an entity type that is equal to the entity type that will be assigned to the token by the relabelling, and if its entity iob label was different from \"B\", its iob label will be set to \"I\". Otherwise, if the token wasn't part of an entity or its iob label was \"B\" or the entity type of its predecessor is different from the entity label that will be assigned to the token it iob label will be assigned to \"B\". For all the other token whose idexes have been added to the `compound_path1`, the iob label will be assigned to \"I\".  Each of this token will be added to the compounds dictionary as key, and the value for each token will be a list with elements are the text of the token, the new iob entity label, the entity label of the token from which compound dependency path they are extracted and the token itself.\n",
    "\n",
    "At the end, the `list_of_token` input list of lists is inspected a second time, and for each token that appears in the `compounds` dictionary the relative list in `list_of_token` is replaced by the value of the key token in the dictionary (a new list that take into account the explained relabelling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compound(list_of_token,doc):\n",
    "    compounds=dict()\n",
    "    for tok in list_of_token:\n",
    "        if tok[-1].ent_type_!=\"\" and ((tok[-1].head.i<tok[-1].i and tok[-1] not in compounds.keys()) or (tok[-1].i<tok[-1].head.i and tok[-1].ent_type_!=tok[-1].head.ent_type_)):\n",
    "            token=tok[-1]\n",
    "            compound_path=[token.i]\n",
    "            get_path_compound(token,compound_path)\n",
    "            compound_path1=sorted(compound_path)\n",
    "            if compound_path1[0]!=0 and token.ent_type_==doc[compound_path1[0]-1].ent_type_ and doc[compound_path1[0]].ent_iob_!=\"B\":\n",
    "                compounds[doc[compound_path1[0]]]=[doc[compound_path1[0]].text, \"I\", token.ent_type_,doc[compound_path1[0]]]\n",
    "            else:\n",
    "                compounds[doc[compound_path1[0]]]=[doc[compound_path1[0]].text, \"B\", token.ent_type_,doc[compound_path1[0]]]\n",
    "            for ind in range(1,len(compound_path1)):\n",
    "                compounds[doc[compound_path1[ind]]]=[doc[compound_path1[ind]].text, \"I\", token.ent_type_, doc[compound_path1[ind]]]\n",
    "    for im in range(0,len(list_of_token)):\n",
    "        if list_of_token[im][-1] in compounds.keys():\n",
    "            list_of_token[im]=compounds[list_of_token[im][-1]]\n",
    "    return list_of_token\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_path_compound` function is used to follow the dependencies from the token taken as input. A check is made on every child of the token: if the dependency of the child is equals to \"compound\" and if the child doesn't belong to any entity or the entity type of the child is the same as the parent, the doc index of the child is added to `compound_path` and the function is recursively called on the child. At the end the `compound_path` list will contain all the doc index of the tokens reached by following the compound dependency from the input token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_compound(token1,compound_path):\n",
    "    for child in token1.children:\n",
    "        if child.dep_==\"compound\":\n",
    "            if child.ent_type_==\"\" or child.ent_type_==token1.ent_type_:\n",
    "                compound_path.append(child.i)\n",
    "                get_path_compound(child,compound_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we try to see how fixing the compound dependency can influence the accuracy, the precision, the recall and the f-measure of the CoNLL 2003 test set. We can see that the performances slightly decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conll_test_compound=my_conll(sentences_test,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.632450</td>\n",
       "      <td>0.544160</td>\n",
       "      <td>0.584992</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.655091</td>\n",
       "      <td>0.513296</td>\n",
       "      <td>0.575589</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.742151</td>\n",
       "      <td>0.666067</td>\n",
       "      <td>0.702054</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.454914</td>\n",
       "      <td>0.270319</td>\n",
       "      <td>0.339124</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.636510</td>\n",
       "      <td>0.490793</td>\n",
       "      <td>0.554234</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p         r         f     s\n",
       "MISC   0.632450  0.544160  0.584992   702\n",
       "PER    0.655091  0.513296  0.575589  1617\n",
       "LOC    0.742151  0.666067  0.702054  1668\n",
       "ORG    0.454914  0.270319  0.339124  1661\n",
       "total  0.636510  0.490793  0.554234  5648"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs_test=conll_test\n",
    "hyps_test_compound=my_conll_test_compound\n",
    "results_test_compound = evaluate(refs_test, hyps_test_compound)\n",
    "pd_tbl_test_compound = pd.DataFrame().from_dict(results_test_compound, orient='index')\n",
    "pd_tbl_test_compound.round(decimals=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.75      0.67      0.71      1668\n",
      "       B-PER       0.64      0.55      0.60       702\n",
      "       I-PER       0.51      0.30      0.38      1661\n",
      "       B-ORG       0.68      0.53      0.59      1617\n",
      "       I-ORG       0.49      0.57      0.53       257\n",
      "       B-LOC       0.26      0.36      0.30       216\n",
      "       I-LOC       0.41      0.52      0.46       835\n",
      "      B-MISC       0.69      0.79      0.74      1156\n",
      "      I-MISC       0.95      0.97      0.96     38323\n",
      "\n",
      "    accuracy                           0.89     46435\n",
      "   macro avg       0.60      0.58      0.58     46435\n",
      "weighted avg       0.89      0.89      0.89     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = [ 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', \n",
    "        'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "y_true_c,y_pred_c=accuracy_token(my_conll_test_compound,conll_test)\n",
    "print(classification_report(y_true_c, y_pred_c,target_names=classes))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
